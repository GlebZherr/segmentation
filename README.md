# segmentation
Web text extraction tool

## Основной pipeline

1. Инструмент разметки: http://<ваш_сервер>:8000/ \
   Для получения размеченной выборки. \
   Flask app находится в директории segmentation/blocks_webapp, приложение запускается скриптом index_blocks.py

2. На основе размеченных данных создается датасет для обучения модели.\
   Сама база банных адресов, блоков, контента страниц и разметки находится в файле data/blocks.sqlite\
   Датасет собирается скриптом make_dataset.py, который находится в дир: features_extract\
   Созданный json-файл будет находится в дир data, имя файла features.json.gzip

3. Валидация модели и подбор гиперпараметров\
   Jupyter notebook: Segclf_domains.ipynb в дир notebooks

4. Обучение модели\
   Jupyter notebook: Segclf.ipynb\
   В результате создается файл модели model_weights_sqrt.cbm

5. Предсказание меток блоков по url-адресу страницы\
   Файл predictor.py в дир: lib\
   В нем находится функция wrapper, которая запрашивает страницу, парсит ее на блоки текста, каждому блоку присваивает метку\
   Пример запуска в секции "if name == '__main__'"\
   Для скорости модель должна быть загружена в память предварительно и оставаться там персистентно\
   Пример предзагрузки модели в скрипте приложения разметки

6. Последний этап, собственно формирование текста новости на основе предсказанной разметки\
   Пример находится в файле load_examples.py в дир features_extract\
   Формирование текста функция get_news_content, пример использования в секции "if name == '__main__'"\
   Эту функцию надо еще доделывать в зависимости от потребности заказчика, но можно использовать и таком виде


## Описание файлов

Используемые файлы/скрипты

blocks_webapp/ \
  index_blocks.py - Запускает веб-сервер приложения разметки данных

lib/ \
  element.py - модуль разбиения html-страницы по блокам\
  http_get.py - модуль запроса страницы и очистки от бесполезной инфы\
  get_features.py - модуль с функциями извлечения фичей из контента страницы\
  predictor.py - модуль загружающий модель и делающий предсказание\
  hyper_search.py - скрипт подбора гипер-параметров

features_extract/\
  make_dataset.py - создает обучающий датасет из размеченных данных\
  load_examples.py - берет из базы рандомные страницы и предиктит блоки и компонует текст новостей для демонстрации функционала в приложении разметки, из него можно только взять функцию, которая компонует текст новости

notebooks/\
  Segclf_domains.ipynb - валидация модели catboost с отложенной выборкой (отложенная выборка делается на доменах, которых нет тренировочной)\
  Segclf_urls.ipynb - валидация модели catboost с отложенной выборкой на основе url\
  Segclf.ipynb - обучение итоговой модели и сохранение файла модели

data/\
  blocks.sqlite - база с рамеченными данными\
  cb_parameters.json - гиперпараметры модели\
  columns.csv - список фичей модели\
  examples.sqlite - база с примерами выхода модели\
  features.json.gzip - обучающий датасет\
  model_weights_sqrt.cbm - сохраненный файл модели
